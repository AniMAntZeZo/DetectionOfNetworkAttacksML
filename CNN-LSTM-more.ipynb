{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, LSTM, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv',\n",
    "              'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv', \n",
    "              'Friday-WorkingHours-Morning.pcap_ISCX.csv', \n",
    "              'Monday-WorkingHours.pcap_ISCX.csv', \n",
    "              'Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv', \n",
    "              'Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv', \n",
    "              'Tuesday-WorkingHours.pcap_ISCX.csv',\n",
    "              'Wednesday-workingHours.pcap_ISCX.csv'\n",
    "              ]  # Замените на ваши пути к файлам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "first_file = True\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path, encoding='latin1')\n",
    "    if first_file:\n",
    "        dfs.append(df)\n",
    "        first_file = False\n",
    "    else:\n",
    "        if df.columns.tolist() == dfs[0].columns.tolist():\n",
    "            dfs.append(df)\n",
    "        else:\n",
    "            raise ValueError(f\"Заголовки в файле {file_path} не совпадают с заголовками первого файла\")\n",
    "\n",
    "data = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    ' Flow Duration', 'Total Length of Fwd Packets', \n",
    "    ' Fwd Packet Length Mean', 'Flow Bytes/s', ' Flow Packets/s', \n",
    "    ' Fwd IAT Mean', ' Bwd IAT Mean', ' Average Packet Size', ' SYN Flag Count', ' Total Backward Packets',\n",
    "    ' ACK Flag Count', ' Packet Length Variance', ' Destination Port', ' Protocol'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[selected_features]\n",
    "\n",
    "y = data[' Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "X.dropna(inplace=True)\n",
    "y = y[X.index]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_scaled = np.expand_dims(X_scaled, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = np.unique(y_train)\n",
    "print(\"Уникальные метки классов:\", unique_labels)\n",
    "print(\"Количество:\", len(unique_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "\n",
    "model.add(Dense(len(unique_labels), activation='softmax')) \n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('modelCNN-more.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=25, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_attack_type(file_path, row_number):\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    selected_features = [\n",
    "    ' Flow Duration', 'Total Length of Fwd Packets', \n",
    "    ' Fwd Packet Length Mean', 'Flow Bytes/s', ' Flow Packets/s', \n",
    "    ' Fwd IAT Mean', ' Bwd IAT Mean', ' Average Packet Size', ' SYN Flag Count', ' Total Backward Packets',\n",
    "    ' ACK Flag Count', ' Packet Length Variance', ' Destination Port', ' Protocol'\n",
    "    ]\n",
    "    \n",
    "    sample_df = data.iloc[[row_number]]\n",
    "    \n",
    "    X_sample = sample_df[selected_features]\n",
    "    \n",
    "    X_sample_scaled = scaler.transform(X_sample)\n",
    "    \n",
    "    X_sample_scaled = np.expand_dims(X_sample_scaled, axis=2)\n",
    "    \n",
    "    prediction = model.predict(X_sample_scaled)\n",
    "    predicted_class = np.argmax(prediction, axis=1)\n",
    "    \n",
    "    predicted_label = label_encoder.inverse_transform(predicted_class)\n",
    "    \n",
    "    return predicted_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BENIGN\n",
    "file_path = 'Wednesday-workingHours.pcap_ISCX.csv'\n",
    "row_number = 6\n",
    "predicted_label = predict_attack_type(file_path, row_number)\n",
    "print(f'Predicted Attack Type: {predicted_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDoS\n",
    "file_path = 'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'\n",
    "row_number = 127328\n",
    "predicted_label = predict_attack_type(file_path, row_number)\n",
    "print(f'Predicted Attack Type: {predicted_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DoS GoldenEye\n",
    "file_path = 'Wednesday-workingHours.pcap_ISCX.csv'\n",
    "row_number = 677132\n",
    "predicted_label = predict_attack_type(file_path, row_number)\n",
    "print(f'Predicted Attack Type: {predicted_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DoS Hulk\n",
    "file_path = 'Wednesday-workingHours.pcap_ISCX.csv'\n",
    "row_number = 297887\n",
    "predicted_label = predict_attack_type(file_path, row_number)\n",
    "print(f'Predicted Attack Type: {predicted_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DoS Slowhttptest\n",
    "file_path = 'Wednesday-workingHours.pcap_ISCX.csv'\n",
    "row_number = 69282\n",
    "predicted_label = predict_attack_type(file_path, row_number)\n",
    "print(f'Predicted Attack Type: {predicted_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DoS slowloris\n",
    "file_path = 'Wednesday-workingHours.pcap_ISCX.csv'\n",
    "row_number = 6560\n",
    "predicted_label = predict_attack_type(file_path, row_number)\n",
    "print(f'Predicted Attack Type: {predicted_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heartbleed\n",
    "file_path = 'Wednesday-workingHours.pcap_ISCX.csv'\n",
    "row_number = 597827\n",
    "predicted_label = predict_attack_type(file_path, row_number)\n",
    "print(f'Predicted Attack Type: {predicted_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PortScan\n",
    "file_path = 'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv'\n",
    "row_number = 154555\n",
    "predicted_label = predict_attack_type(file_path, row_number)\n",
    "print(f'Predicted Attack Type: {predicted_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_attack_types(file_path, start_row, end_row):\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    selected_features = [\n",
    "    ' Flow Duration', 'Total Length of Fwd Packets', \n",
    "    ' Fwd Packet Length Mean', 'Flow Bytes/s', ' Flow Packets/s', \n",
    "    ' Fwd IAT Mean', ' Bwd IAT Mean', ' Average Packet Size', ' SYN Flag Count', ' Total Backward Packets',\n",
    "    ' ACK Flag Count', ' Packet Length Variance', ' Destination Port', ' Protocol'\n",
    "    ]\n",
    "    \n",
    "    sample_df = data.iloc[start_row:end_row]\n",
    "    \n",
    "    X_sample = sample_df[selected_features]\n",
    "    \n",
    "    X_sample_scaled = scaler.transform(X_sample)\n",
    "    \n",
    "    predictions = model.predict(X_sample_scaled)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    predicted_labels = label_encoder.inverse_transform(predicted_classes)\n",
    "    \n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 100\n",
    "total_time = 0\n",
    "\n",
    "file_path = 'Wednesday-workingHours.pcap_ISCX.csv'\n",
    "row_number = 6\n",
    "\n",
    "single_sample = np.array([X_test[0]])\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    total_time += timeit.timeit(lambda: predict_attack_type(file_path, row_number), number=1)\n",
    "\n",
    "average_inference_time = total_time / num_iterations\n",
    "print(f\"Average inference time: {average_inference_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
